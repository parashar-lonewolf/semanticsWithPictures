{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Genome: Spatial sematic Analysis, an attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from visual_genome import api as vg ## visual_genome is the name of the toolbar folder\n",
    "from PIL import Image as PIL_Image\n",
    "import requests\n",
    "try:\n",
    "    from StringIO import StringIO ## python 2\n",
    "except ImportError:\n",
    "    from io import StringIO ## python 3\n",
    "import pandas as pd\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "# In the VG datset-annotation bundle, spatial languages can be found in any kind of region description or \n",
    "# image description but then I realized that since spatial sematics will not appear as attributes since they will \n",
    "# be Prepositional Phrases, it has to be only present as a relationship, so now we will look only into the \n",
    "# various relationships used in each image annotation and break them into their POS tags, to check \n",
    "# across text for a few categories of spatial parameters.\n",
    "\n",
    "### for the 1st image\n",
    "## region description \n",
    "reg_desc = vg.get_region_descriptions_of_image(id=1)\n",
    "print(len(reg_desc))\n",
    "## relationships within an image\n",
    "reg_rel = vg.get_scene_graph_of_image(id=1)\n",
    "print(len(reg_rel.relationships))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA FORMAT OF POS\n",
    "# text      : lemma_ : pos_ : tag_ : dep_  : shape_ : is_alpha : is_stop\n",
    "# the       : the    : DET  : DT   : det   : xxx    : True     : True\n",
    "# clock     : clock  : NOUN : NN   : nsubj : xxxx   : True     : False\n",
    "# is        : be     : VERB : VBZ  : ROOT  : xx     : True     : True\n",
    "# green     : green  : ADJ  : JJ   : acomp : xxxx   : True     : False\n",
    "# in        : in     : ADP  : IN   : prep  : xx     : True     : True\n",
    "# colour    : colour : NOUN : NN   : pobj  : xxxx   : True     : False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. POS tagging relationships of first 10 images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## collecting relationships into\n",
    "relationships = []\n",
    "for i in range(1,10):\n",
    "    reg_grph = vg.get_scene_graph_of_image(id=i)\n",
    "    relationships.append(reg_grph.relationships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## storage arrays\n",
    "## FORMAT:\n",
    "## POS_table = [[[a,b,c,a,s],[a,s,d,q,w]].....,\n",
    "##              [[q,w,e,y,u],[e,w,q,b,n]].....,\n",
    "##               ...............\n",
    "##              [[6,6,6,0,1],[7,7,7,1,0]]....]\n",
    "POS_table = []\n",
    "\n",
    "## POS tagging of all relationships to do a hollistic analysis\n",
    "for i in range(1,10):\n",
    "    # Image's scene graph object\n",
    "    \n",
    "    # POS tagging all image object/subject relationships\n",
    "    \n",
    "    for relation in :\n",
    "        sent = str(relation).split(\": \")[1]\n",
    "        print(sent)\n",
    "        doc = nlp(sent)\n",
    "#         print(\"text:lemma:pos:tag:dep:shape:is_alpha:is_stop\") ## POS format\n",
    "        sent_pos = []\n",
    "        rel_count = 0\n",
    "        for token in doc:    \n",
    "#             print(token.text,':', token.lemma_,':', token.pos_,':', token.tag_,':', \n",
    "#                   token.dep_,':',token.shape_,':', token.is_alpha,':', token.is_stop)\n",
    "            sent_pos.append([token.text,token.lemma_,token.pos_,token.tag_,token.dep_])\n",
    "        POS_table.append(sent_pos)\n",
    "        rel_count+=1\n",
    "#         break ## per relationship \n",
    "#     break ## per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Spatial language data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the image data\n",
    "Next, we will get some data about the image. We specifically want to know the image's url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = vg.GetImageData(id=image_id)\n",
    "# print \"The url of the image is: %s\" % image.url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the region descriptions\n",
    "Now, let's get all the region descriptions for this image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regions = vg.GetRegionDescriptionsOfImage(id=image_id)\n",
    "# print \"The first region descriptions is: %s\" % regions[0].phrase\n",
    "# print \"It is located in a bounding box specified by x:%d, y:%d, width:%d, height:%d\" % (regions[0].x, regions[0].y, regions[0].width, regions[0].height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing some regions\n",
    "Now, we will visualize some of the regions. The x,y coordinates of a region refer to the top left corner of the region. Since there are many regions, we will only visualize the first 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.gcf()\n",
    "# fig.set_size_inches(18.5, 10.5)\n",
    "# def visualize_regions(image, regions):\n",
    "#     response = requests.get(image.url)\n",
    "#     img = PIL_Image.open(StringIO(response.content))\n",
    "#     plt.imshow(img)\n",
    "#     ax = plt.gca()\n",
    "#     for region in regions:\n",
    "#         ax.add_patch(Rectangle((region.x, region.y),\n",
    "#                                region.width,\n",
    "#                                region.height,\n",
    "#                                fill=False,\n",
    "#                                edgecolor='red',\n",
    "#                                linewidth=3))\n",
    "#         ax.text(region.x, region.y, region.phrase, style='italic', bbox={'facecolor':'white', 'alpha':0.7, 'pad':10})\n",
    "#     fig = plt.gcf()\n",
    "#     plt.tick_params(labelbottom='off', labelleft='off')\n",
    "#     plt.show()\n",
    "# visualize_regions(image, regions[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
